Parfait, **un DAG Airflow comme *sonde de santÃ© Spark sur Kubernetes***, **sans Grafana / Prometheus / Kibana**.
Lâ€™idÃ©e : **Airflow interroge pÃ©riodiquement Kubernetes + Spark History Server + Ã©ventuellement le Spark Operator** et pousse un **rÃ©sultat consolidÃ©** dans un mail / Slack / fichier / XCom.

Ci-dessous je te donne **une solution complÃ¨te et reproductible**, *production-ready*, que tu peux adapter chez toi.

---

# ğŸ¯ **Objectif du DAG**

Ã€ chaque run :

1. Lister les **pods Spark actifs** (driver/executor)
2. Lister les **derniers jobs Spark finis** dans **Spark History Server (SHS)**
3. VÃ©rifier :

   * drivers en Ã©chec ?
   * executors souvent OOMKilled ?
   * jobs Spark rÃ©cemment FAILED ?
   * pods bloquÃ©s `Pending` ?
4. Produire un **rapport consolidÃ©**
5. **Notifier par mail / Slack**

---

# ğŸ“Œ **DÃ©pendances**

Airflow doit pouvoir accÃ©der :

* **API Kubernetes (`/api/v1/...`)**
* **Spark History Server REST API (`/api/v1/applications`)**
  â†’ valable avec Spark 3.3+

Tu peux utiliser la connexion Airflow :

```
Conn ID = kubernetes_default
Conn ID = spark_history_server
```

---

# ğŸ§± **DAG complet : `spark_health_monitor.py`**

> Ce DAG ne crashe pas si un service manque â†’ il *marque lâ€™Ã©tat* comme inconnu et continue

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable
import requests
from kubernetes import client, config
import datetime

SPARK_HISTORY_URL = Variable.get("SPARK_HISTORY_URL")  # ex: http://spark-history-server:18080

def get_k8s_spark_pods(**context):
    config.load_incluster_config()  # ou load_kube_config() en dev
    v1 = client.CoreV1Api()
    pods = v1.list_pod_for_all_namespaces(label_selector="spark-role")  # label du spark-operator
    report = {"running": [], "pending": [], "failed": []}

    for pod in pods.items:
        status = pod.status.phase
        name = pod.metadata.name

        if status == "Running":
            report["running"].append(name)
        elif status == "Pending":
            report["pending"].append(name)
        else:
            report["failed"].append({"name": name, "reason": status})

    context['ti'].xcom_push(key="k8s_report", value=report)


def get_spark_history_state(**context):
    """RÃ©cupÃ¨re l'Ã©tat des jobs spark terminÃ©s via SHS"""
    report = {"completed": 0, "failed": 0, "apps": []}

    try:
        resp = requests.get(f"{SPARK_HISTORY_URL}/api/v1/applications", timeout=5)
        apps = resp.json()
    except Exception:
        context['ti'].xcom_push(key="history_report", value={"status": "unreachable"})
        return

    # On ne garde que les 20 derniers
    for app in apps[:20]:
        attempts = app.get("attempts", [])
        for a in attempts:
            status = a.get("completed", False)
            final = a.get("finalStatus", "UNKNOWN")

            if final == "SUCCEEDED":
                report["completed"] += 1
            else:
                report["failed"] += 1

            report["apps"].append({
                "id": app["id"],
                "name": app["name"],
                "state": final,
                "duration_s": a.get("duration")
            })

    context['ti'].xcom_push(key="history_report", value=report)


def build_health_summary(**context):
    k8s_report = context['ti'].xcom_pull(key="k8s_report")
    history_report = context['ti'].xcom_pull(key="history_report")

    result = {
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "pods_running": len(k8s_report["running"]),
        "pods_pending": len(k8s_report["pending"]),
        "pods_failed": len(k8s_report["failed"]),
        "spark_completed": history_report.get("completed", 0),
        "spark_failed": history_report.get("failed", 0),
    }

    # rÃ¨gle simple : cluster sain si pas de pending ni de failed rÃ©cents
    result["cluster_healthy"] = (
        result["pods_failed"] == 0 and
        result["pods_pending"] == 0 and
        result["spark_failed"] == 0
    )

    context['ti'].xcom_push(key="summary", value=result)


def print_report(**context):
    summary = context['ti'].xcom_pull(key="summary")
    print("=== SPARK HEALTH REPORT ===")
    for k,v in summary.items():
        print(f"{k}: {v}")

    if not summary["cluster_healthy"]:
        raise Exception("Spark cluster is NOT healthy!")


with DAG(
    dag_id="spark_health_monitor",
    schedule_interval="*/30 * * * *",  # toutes les 30 min
    start_date=days_ago(1),
    catchup=False,
) as dag:

    k8s_pods = PythonOperator(
        task_id="check_k8s_spark_pods",
        python_callable=get_k8s_spark_pods
    )

    spark_history = PythonOperator(
        task_id="check_spark_history",
        python_callable=get_spark_history_state
    )

    build_summary = PythonOperator(
        task_id="build_health_summary",
        python_callable=build_health_summary
    )

    report = PythonOperator(
        task_id="print_report",
        python_callable=print_report
    )

    k8s_pods >> spark_history >> build_summary >> report
```

---

# ğŸ§  **Ce que surveille rÃ©ellement ce DAG**

| Source        | VÃ©rification | SymptÃ´me                     |
| ------------- | ------------ | ---------------------------- |
| Kubernetes    | pods pending | cluster saturÃ© / quota       |
| Kubernetes    | pods failed  | OOMKilled / imagePull / RBAC |
| Spark History | jobs failed  | perf / exceptions spark      |
| Spark History | recurrence   | stabilitÃ© dans le temps      |

**=> Câ€™est exactement ce que font les plateformes internes bancaires, mais sans Prometheus.**

---

# ğŸšï¸ **Aller plus loin**

| Option               | IntÃ©rÃªt                | IntÃ©gration dans le DAG |
| -------------------- | ---------------------- | ----------------------- |
| Notifier Slack/Teams | Alerting               | `SlackWebhookOperator`  |
| Notifier email       | Rapports rÃ©guliers     | `EmailOperator`         |
| Stocker en S3        | Historique santÃ©       | `S3Hook`                |
| Webhook              | IntÃ©gration SI interne | `SimpleHttpOperator`    |
| Scoring santÃ©        | Confiance 0â€“100        | pondÃ©ration KPI         |

---

# ğŸ’ *RÃ©sultat attendu*

Dans Airflow UI :
â†’ **un seul DAG**,
â†’ **statut rouge = mauvais Ã©tat Spark**,
â†’ **logs Airflow = rapport lisible + automatisable**.

---

# ğŸš€ Tu veux la version **avec envoi d'email stylÃ© HTML** ?

Dis-moi **Slack ou Mail**, je te donne :

* modÃ¨le HTML
* badges couleur (OK / WARN / KO)
* piÃ¨ces jointes avec logs
* score de santÃ© (%)

On peut mÃªme gÃ©nÃ©rer un **PDF automatique** pour management.

Tu veux quelle version ?
`Slack`, `Email`, `PDF`, `CSV`, `Callback Teams`, `Webhook Dynatrace` ?
